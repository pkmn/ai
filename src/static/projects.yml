- name: Technical Machine
  site: http://doublewise.net/pokemon/
  active: 2010 # September 6
  license: BSL-1.0
  source: https://github.com/davidstone/technical-machine
  engine: Custom
  language: C++
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
    - name: Pokémon Online
      url: https://github.com/po-devs/pokemon-online
    - name: Pokémon Lab
      url: https://pokemonlab.com/downloads
    - name: Shoddy Battle
      url: https://pokemonlab.com/downloads
    - name: Pokémon NetBattle
      url: https://pokemon-netbattle.en.softonic.com/
- name: Shanai
  active: [2011, 2011] # March 21
  source: https://github.com/nixeagle/shanai
  engine: Custom
  language: Common Lisp
  platform:
    - name: Pokémon Online
      url: https://github.com/po-devs/pokemon-online
  description: |
    One of the earliest known examples of a competitive Pokémon AI, Shanai was most notable for the
    creation of the ["Shanai
    Cup"](https://web.archive.org/web/20110706011535/http://pokemon-online.eu/forums/showthread.php?6273),
    a simplified variant of Pokémon that hoped to serve as the testbed for AI development. Shanai
    only supported this restricted Generation V variant and its [core
    algorithm](https://github.com/nixeagle/shanai/blob/20f11841/PO/Client/client.lisp#L370) can be
    recognized as an extension of the rudimentary [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer)
    benchmark. The project also contains the [beginnings of a reverse damage
    calculator](https://github.com/nixeagle/shanai/blob/20f11841/Pokemon/formulas.lisp), though
    ultimately the project was abandoned before this feature was incorporated.
- active: [2014, 2016] # October 20
  license: MIT
  source: https://github.com/vasumv/pokemon_ai
  engine: Custom
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: "0.0.1"
    url: https://github.com/vasumv/pokemon_ai/releases/tag/0.0.1
  description: |
    An early agent targeting the [Generation VI OU
    format](https://www.smogon.com/dex/xy/formats/ou/) that leveraged [depth
    2](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/game.py#L25) [determinized and
    sequentialized pure minimax search with alpha-beta
    pruning](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/agent.py) and a
    [hand-crafted evaluation
    function](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/gamestate.py#L46) to
    play with [predefined teams](https://github.com/vasumv/pokemon_ai/tree/0adbf47d/teams). While
    eschewing expectiminimax or a simultaneous-move minimax, the AI experimented between both
    optimistic and pessimistic sequentializations, seemingly preferring the latter.

    The AI took an unorthodox approach towards integrating with the Pokémon Showdown platform,
    relying on a [scriptable headless browser](https://phantomjs.org/) instead of the more obvious
    [WebSocket](https://en.wikipedia.org/wiki/WebSocket) API, complicating
    [connection](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdown_ai/browser.py) and
    [parsing](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/log.py), possibly due to
    misunderstandings or platform limitations that are no longer relevant.

    The agent relied on a custom engine for [simulating
    actions](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/simulator.py#L228),
    replete with simplified [move
    handler](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/handlers.py) and [damage
    calculator](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/moves.py#L63)
    implementations. The engine removed all stochastic elements (_e.g._, by ignoring accuracy checks
    or the contribution of damage rolls) and as such the AI's game state was
    [patched](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/showdownai/showdown.py#L149)
    between actions to better keep it sync with the true server state.

    The AI featured relatively sophisticated team prediction capabilities:
    [movesets](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/data/poke3.json) were
    [scraped](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/smogon/smogon.py) from Smogon
    University, with the agent falling back on data from the prior generation where necessary. These
    served as the basis for an opponent's predicted set, though with the set's moves instead
    replaced with the moves that occur most frequently as computed from a [replay
    database](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/log_scraper/database.py) consisting
    of logs from [top ladder
    players](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/log_scraper/log_scraper.py). This
    database was also used to produce [$`P(Move \mid
    Move)`](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/data/graph_move.json) and [$`P(Move
    \mid Species \land
    Move)`](https://github.com/vasumv/pokemon_ai/blob/0adbf47d/data/graph_poke3.json) correlations
    that allowed the agent to use Bayesian inference as further moves get revealed to better predict
    an opponent's moveset.
- name: Percymon
  active: [2014, 2017] # October 21
  paper: Ho:2014
  source: https://github.com/rameshvarun/showdownbot
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: pokemon-showdown-client
      url: https://github.com/smogon/pokemon-showdown-client
  language: JavaScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    The first competitive Pokémon AI agent to be described in a paper, Percymon targeted the
    Generation VI Random Battle format. The bot relied on pure [depth
    2](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#L286) pessimistic
    sequentialized
    [minimax](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js) with
    alpha-beta pruning and [move
    ordering](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#L339-343)
    (_e.g._, preferring to explore super effective or status moves before those which are not very
    effective) with a [hand crafted evalutation
    function](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#L263)
    based on an early verison of [Technical Machine](/projects/#TechnicalMachine)'s
    [features](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#20-L57)
    and [weights](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/weights.js). Percymon
    also included several [hardcoded special cases for certain
    moves](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#L352-366)
    which have intricate game mechanics that were hard for the agent to properly evaluate.

    Due to Percymon being built around an unmodified copy of Pokémon Showdown it was unable to
    easily make use of the expectiminimax algorithm. Furthermore, while the agent was mostly
    deterministic, the damage and turn simulations its ran via Pokémon Showdown did not fix a random
    seed and thus the bot's policies could exhibit a small amount of non-determinism. The Pokémon
    Showdown engine was found to be a major performance bottleneck, with the
    [cloning](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/clone.js)  of the engine's
    state between simulations cited as the major limiting factor on depth as even with pruning turns
    could frequently take in excess of 40s to evaluate.

    Percymon chose to simplify the action space by [always mega
    evolving](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/battleroom.js#L749) when
    available as a form of forward pruning. It also chose to simplify the question of imperfect
    information by assuming that an [opponent's unrevealed Pokémon do not
    exist](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/battleroom.js#L125)  and that
    [an opponent has all possible
    moves](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/battleroom.js#L129) from their
    [random move pool](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/formats-data.js)
    until [all of their moves have been
    revealed](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/battleroom.js#L185-L204) (as
    opposed to leveraging usage stats). In order to work around the increased move acion space
    resulting from this approach, Percymon [only looked at the top
    10](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#438) opponent
    choices (_i.e_, forward pruning via beam search).

    Percymon planned to use
    [TD-learning](https://github.com/rameshvarun/showdownbot/blob/00dcfcca/bots/minimaxbot.js#60-L136)
    for the evaluation function weights and considered supporting non-random formats by adding in a
    usage statistics-backed Bayesian network for team prediction coupled with a CSP solver to build
    robust teams.
- name: Leftovers Again
  framework: true
  active: [2015, 2021] # September 1
  source: https://github.com/dramamine/leftovers-again
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: pokemon-showdown-client
      url: https://github.com/smogon/pokemon-showdown-client
    - name: damage-calc
      url: https://github.com/smogon/damage-calc
  language: JavaScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: Froslass
    url: https://github.com/dramamine/leftovers-again/releases/tag/v0.9.6
  description: |
    The first framework to be published for competitive Pokémon artificial intelligence, Leftovers
    Again aimed to lower the barrier of entry for beginners. Supporting Generation VII (both random
    and team-based formats), Leftovers Again provided a
    [client](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/src/model) and [damage
    calculator](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/src/game) along with
    instructions for getting started and infrastructure for running and debugging agents. Leftovers
    Again featured support for [round-robin
    tournaments](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/scripts/roundrobin.js)
    against the various (mostly rules-based) [sample
    bots](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/src/bots) it bundled, including
    [$`RandomPlayer`](/glossary#RandomPlayer)
    ("[randumb](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/src/bots/randumb/index.js)")
    and [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer)
    ("[stabby](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/src/bots/stabby/stabby.js)")
    implementations. Leftovers Again also included [replay
    scraping](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/scripts/replay-saver.js)
    and
    [processing](https://github.com/dramamine/leftovers-again/blob/ddcbdbaa/scripts/replay-processor.mongo.js)
    functionality, the latter of which could be used to produce $`P(Move \mid Species \land Move)`
    bigrams.
- name: Bill's PC
  active: [2015, 2016] # September 8
  source: https://github.com/sobolews/BillsPC
  engine: Custom
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    Bill's PC was a promising but ultimately unrealized and overlooked attempt at implementing a
    competitive Pokémon AI based on more contemporary game theory techniques. Bill's PC was written
    in Python and targeted the Generation VI Random Battle format, though hoped to eventually
    support Generation VII and recognized a rewrite in C++ would likely be necessary to achieve the
    performance required for a high level of play.

    Bill's PC featured several novel ideas --- it was the first known example of producing usage
    statistics for the Random Battle formats by [leveraging Pokémon Showdown's team generation
    logic](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/showdowndata/js/getNRandomTeams.js), a
    technique independently rediscovered and popularized by
    [`pkmn/randbats`](https://github.com/pkmn/randbats) half a decade later. However, [Bill's PC's
    processed statistics](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/showdowndata/miner.py)
    were significantly richer than those offered by `pkmn/randbats`, approaching the depth of [those
    offered by Smogon University](https://www.smogon.com/stats/) for non-random formats.

    Additionally, Bill's PC demonstrated the ability to reuse its own high-quality [custom
    engine](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/battle/battleengine.py) implementation
    for damage calculation, overriding certain functions to force specific damage rolls or critical
    hit outcomes in order to [compute damage
    ranges](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/bot/battlecalculator.py) --- a
    primitive but powerful initial version of the functionality that later could be found in
    [`libpkmn`](https://github.com/pkmn/engine).

    Bill's PC's [client](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/bot/battleclient.py)
    included a distinct [opponent
    representation](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/bot/foeside.py) tracking known
    and possible moves and attempted to [deduce an opponent's Hidden Power
    type](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/bot/battleclient.py#L226),
    deterministically [filled in unknown
    attributes](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/AI/rollout.py#L53) with their most
    likely value based on the usage statistics, and also tried to leverage typing information to
    determine which unknown [species of Pokémon might "balance" an opponent's team
    composition](https://github.com/sobolews/BillsPC/blob/d1e2fd8c/AI/rollout.py#L191).

    Bill's PC's roadmap showed plans to implement "SM-MCTS for stacked matrix games ([Tak
    2014](/research/#Tak:2014){.subtle}) and/or LP solutions for Nash equilibria with learned
    valuation functions", but despite the solid foundational work on the client and engine, the
    project appears to have been abandoned shortly after the initial skeletons of an agent were
    implemented.
- name: Deep Red
  active: [2016, 2016] # April 2
  source: https://github.com/TwitchPlaysPokemon/deepred
  engine: Custom
  language: Python
  platform:
    - name: Pokémon Anniversary Crystal
      url: https://rainbowdevs.com/title/crystal/
  description: |
    A competitive Pokémon battling AI that existed to serve as the in-game computer opponent for a
    ROM-hack when run in a custom emulator, Deep Red is a typical rules-based agent with a
    [hand-crafted evaluation
    function](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L678). Due to
    the unique platform it was built for it could leverage perfect information to simplify its
    implementation, though targeting an in-game battle system also meant it had to support the
    additional action of using items in battle, a game element not present in the multiplayer link
    battle system online play is based around. Deep Red's algorithm was mostly deterministic,
    primarily leveraging a [damage
    calculator](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L283) that
    computed _expected_ damage, though it also featured a notion of exploring
    "[combos](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L1201)"
    computed via [limited
    lookahead](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L34) and
    contained basic implementations of a [killer move
    heuristic](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L1921) and
    its inverse, [changing behavior of the AI when its own HP was
    low](https://github.com/TwitchPlaysPokemon/deepred/blob/350fe081/oldai/AI.py#L1719).
- active: [2016, 2023] # December 8
  license: MIT
  source: https://github.com/Synedh/showdown-battle-bot
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- identifier: Xu & Verbugge
  active: [2016, 2016]
  paper: Xu:2016
- name: Showdown AI competition
  framework: true
  active: [2017, 2018] # February 28
  paper: Lee:2017
  source: https://github.com/scotchkorean27/showdownaiclient
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: PokéAI
  active: [2017, 2022] # March 14
  license: MIT
  source: https://github.com/select766/pokeai
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: [Python, TypeScript]
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: 2020年9月(技術書典9)
    url: https://github.com/select766/pokeai/releases/tag/book-202009
- name: CynthiAI
  active: [2017, 2017] # May 23
  source: https://github.com/Sisyphus25/CynthiAI
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: JavaScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    CynthiAI targeted the Generation VII Random Battle format using a mixture of heuristics and
    [depth 1](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L769) [pure
    optimistic
    minimax](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L577-L742) search
    with a [hand-crafted evaluation
    function](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L187-575).
    CynthiAI [deterministically used the highest accuracy move that would knock out its
    opponent](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L163-L182) (killer
    move heuristic) otherwise selected the most damaging move (ignoring accuracy), finally falling
    back on the result of its minimax search. Like [Percymon](/projects/#Percymon), to better
    account for game mechanics and prevent suboptimal decisions, CynthiAI applied further [hardcoded
    modifiers](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L777-L794) to the
    minimax result. In lieu of team prediction via usage statistics, CynthiAI worked off of an
    opponent's [known
    moves](https://github.com/Sisyphus25/CynthiAI/blob/37dd2e41/CynthiAgent.js#L123) if enough were
    known otherwise defaulted to random move pool for the species.
- name: SutadasutoIA
  active: [2017, 2018]
  paper: Rill-García:2017
  source: https://github.com/Sutadasuto/SutadasutoIA
  license: MIT
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- identifier: Rill-García
  active: [2018, 2018]
  paper: Rill-García:2018
  language: Python
- active: [2018, 2018] # Feb 9
  source: https://github.com/DanielAlcocerSoto/Pokemon-Python
  engine: Custom
  language: Python
- name: PokeML
  active: [2018, 2019] # July 13
  license: MIT
  source: https://github.com/pokeml/pokemon-agents
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: pokemon-showdown-client
      url: https://github.com/smogon/pokemon-showdown-client
  language: JavaScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: "0.2.5"
    url: https://www.npmjs.com/package/pokemon-env/v/0.2.5
  description: |
    The PokeML project contained a collection of several Generation VII Pokémon battle agents that
    [simulated battles via Pokémon
    Showdown](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/core/team-aware-simulation-agent.js#L42)
    and whose client representations tracked state using an [extraction from its
    client](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/tracking). PokeML's agents included:

    - A basic [$`RandomPlayer`](/glossary#RandomPlayer)
    ([`RandomAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/random-agent.js))
    and a $`RandomPlayer(switch=\widetilde{0\%}, mega=100\%)` implementation
    ([`SemiRandomAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/random-agent.js)).
    - The classic  [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer)
    ([`MaxDamageAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/max-damage-agent.js))
    and a more simplified version that only considered a move's base power
    ([`MaxBasePowerAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/max-base-power-agent.js)).
    - [`SimAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/sim-agent.js),
    an agent which performed a one-turn lookahead by simulating each possible joint action,
    [evaluating](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/sim-agent.js#L311-L336)
    the new state based purely on the sum of HP percentages of Pokémon on each side, using
    [`lrsnash` to solve the computed payoff
    matrix](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/sim-agent.js#L91) and
    making a decision based on a [weighted random sample from this
    solution](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/sim-agent.js#L129).
    - [`ChecksSwitchAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/checksswitch-agent.js)
    which chose actions based on hard-coded [checks and counters
    table](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/pokemon-agents/data/compTest.json)
    and the typechart, switching to whichever Pokémon on the team has the best matchup versus the
    current opponent Pokémon (or attacking randomly if already out).
    - [`TestAgent`](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/agents/test-agent.js),
    a work-in-progress agent utilizing machine learning which targeted a [reduced
    conmeta](https://github.com/pokeml/pokemon-agents/src/agents/test-agent.js#L11-36) to make
    [encoding](https://github.com/pokeml/pokemon-agents/blob/318b7d3d/src/core/encoder.js) the game
    states into fixed-size numeric vectors more tractable.
- identifier: Kalose, Kaya & Kim
  active: [2018, 2018]
  paper: Kalose:2018
  engine: Custom
  language: Python
  description: |
    Kalose _et al._ implemented a custom deterministic simulator for the Generation I Challenge Cup
    format and evaluated the efficacy of an agent trained on self-play utilizing Q-learning. The
    agent's feature vector was limited to just the active Pokémon from either side's types and
    corresponding "HP bucket" (an abstraction of a Pokémon's remaining HP percentage into 10 evenly
    distributed bins), deliberately avoiding any information about either side's inactive party due
    to performance concerns. In addition to the large terminal reward based on the battle's final
    result, smaller intermediate rewards were provided to the agent based on the remaining HP for
    both sides. A softmax exploration strategy was seen to outperform an ε-greedy one versus
    [$`RandomPlayer`](/glossary#RandomPlayer), though overall playing strength was poor compared to
    a benchmark minimax solution. Suggestions for future work included introducing eligibility
    traces and expanding the size of the feature vector.
- active: 2018 # August 21
  license: MIT
  source: https://github.com/taylorhansen/pokemonshowdown-ai
  engine:
    - name: '@pkmn/sim'
      url: https://github.com/pkmn/ps/tree/main/sim
  language: [Python, TypeScript]
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- active: [2018, 2018] # December 3
  paper: Chen:2018
  license: MIT
  source: https://github.com/kvchen/showdown-rl
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    Chen _et al._ trained an agent using proximal policy optimization (PPO), anticipating an
    approach using Q-learning would struggle to learn a value function for a game as complicated as
    Pokémon. While their bot targeted the Generation I Random Battle format the agent was
    generalized and could be trained on arbitrary formats.

    The agent utilized [Tensorflow](https://www.tensorflow.org/), the [PPO implementation from
    OpenAI](https://github.com/kvchen/showdown-rl/blob/8cb4fd90/ppo/ppo.py) and a custom [OpenAI
    Gym](https://github.com/openai/gym) environment that works with a modified Pokémon Showdown
    server. The server was relaxed to provide additional information about
    [features](https://github.com/kvchen/showdown-rl-server/blob/f31875ee/src/battle/getFeatures.js)
    and to [determine valid
    actions](https://github.com/kvchen/showdown-rl-server/blob/f31875ee/src/battle/getValidActions.js#L19)
    ahead of time which are used to mask off the output from the RL network to force the logits of
    invalid actions to negative infinity before applying the softmax.

    While Chen _et al._ discussed using the `node2vec` algorithm to [create embeddings for various
    species](https://github.com/kvchen/showdown-rl-notebooks/blob/85c266d4/embeddings/Pokemon%20Embeddings%20Using%20Node2Vec.ipynb),
    it seems as though a basic one-hot encoding for each Pokémon's attributes was [ultimately
    used](https://github.com/kvchen/gym-showdown/blob/c5cdcf84/gym_showdown/envs/showdown_env.py#LL141-L179).
    The model's network architecture featured 3 fully-connected layers each with 512 units and a
    ReLU activation with only terminal states being rewarded. The agent trained against
    $`RandomPlayer(\widetilde{100\%})`, $`RandomPlayer(0\%)`, and a depth 1 [minimax
    agent](https://github.com/kvchen/showdown-rl/blob/8cb4fd90/agents/minimax.py) with alpha-beta
    pruning with an evaluation function that compared the difference in the sum of HP percentages
    for the full team of each side.

    The agent eventually learned a policy which preferentially chose the move in the 4th moveslot,
    calling into question the results. Chen _et al._ reveal they largely bottlenecked by
    computational resources, in particular repeatedly
    [cloning](https://github.com/kvchen/showdown-rl-server/blob/f31875ee/src/battle/clone.js) the
    simulator state, and their ideas for future work included self-play and improved visualization
    to enhance tuning and debugging.
- identifier: Ihara, Imai, Oyama & Kurihara
  active: [2018, 2018]
  paper: Ihara:2018
  description: |
    Ihara _et al._ compared information set MCTS (ISMCTS) to determinized conventional MCTS in the
    Generation VI Pokémon BSS format. Using a custom simulator and a fixed pool of six Pokémon
    combined with a client representation that leveraged usage statistics from [Pokémon Global
    Link](https://bulbapedia.bulbagarden.net/wiki/Pok%C3%A9mon_Global_Link) and heuristics fill in
    unknown information for an opponent's team, Ihara _et al._ found that ISMCTS was superior to
    determinized MCTS when the number of iterations were fixed. Both approaches were significantly
    weaker than agents leveraging MCTS with perfect information which were used as a control, though
    the gap decreased in further experiments where the battle rules were simplified.
- active: [2019, 2019] # February 10
  source: https://github.com/hsahovic/reinforcement-learning-pokemon-bot
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    Inspired by [`Synedh/showdown-battle-bot`](#Synedh/showdown-battle-bot), the
    `hsahovic/reinforcement-learning-pokemon-bot` is the precursor to [poke-env](#poke-env). Perhaps
    most notable as the
    [origin](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/tree/25a04789/showdown-improvements)
    of various patches to Pokémon Showdown that were eventually
    [upstreamed](https://github.com/smogon/pokemon-showdown/pull/7618) Pokémon Showdown to make it
    more suitable for reinforcement learning training, the project uses [Keras](https://keras.io/)
    and [Tensorflow](https://www.tensorflow.org/) to experiment with reinforcement learning
    approaches. The project features a [client representation](
    https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789/src/environment/battle.py)
    that is used to produce a [description of the
    state](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789/src/environment/battle.py#L445)
    to be fed into models and contains support for
    [self-play](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789master/src/players/base_classes/model_manager.py#L269)
    and concurrent training. The project contains two models built with a fully-connected
    multi-layer perceptron architecture - an incomplete [larger
    model](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789/src/players/fully_connected_random_model.py)
    that utilizes the full feature set and a functional model
    [model](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789/src/players/policy_network.py)
    that uses the Actor-Critic method with a pruned number of features. The latter agent's
    [reward](https://github.com/hsahovic/reinforcement-learning-pokemon-bot/blob/25a04789/src/players/policy_network.py#L238C8-L238C63)
    depends on the difference in respective total party hit points.
- name: Showdown AI Bot
  active: [2019, 2019] # March 8
  license: MIT
  source: https://github.com/JJonahJson/Showdown-AI-Bot
  engine: Custom
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: "PSB: Gennaro's Revenge"
    url: https://github.com/JJonahJson/Showdown-AI-Bot/releases/tag/v1.0
- name: Metagrok
  active: [2019, 2019] # June 16
  paper: Huang:2019
  license: MIT
  source: https://github.com/yuzeh/metagrok
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- site: https://www.smogon.com/forums/threads/3648979/
  active: 2019 # March 24
  license: GPL-3.0
  source: https://github.com/pmariglia/showdown
  engine:
    - name: Custom
      url: https://github.com/pmariglia/showdown/blob/master/ENGINE.md
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: 0 ERROR # April 20
  site: https://pkmn.cc/0-ERROR
  active: 2019
  license: CC-BY-NC-ND-4.0
  source: https://github.com/pkmn/0-ERROR
  engine:
    - name: pkmn
      url: https://github.com/pkmn/engine
    - name: EPOké
      url: https://github.com/pkmn/EPOke
  language: Zig
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    0 ERROR exists as the theoretical motivating use case behind the [pkmn](https://pkmn.cc)
    project. While the actual agent is currently vaporware, pkmn has been building foundational
    Pokémon infrastructure that has been used within multiple other Pokémon AI projects:

    - [pkmn/ps](https://github.com/pkmn/ps): Pokémon Showdown's simulator and client broken up into
    composable libraries.
    - [data.pkmn.cc](https://data.pkmn.cc): An aggregated source exposing all of Smogon University's
    data in machine-readable form.
    - [pkmn/stats](https://github.com/pkmn/stats): A framework for efficient Pokémon Showdown battle
    log processing and usage stats generation.
    - [pkmn/engine](https://github.com/pkmn/engine): A minimal, complete, battle simulation engine
    optimized for performance.
    - [EPOKé](https://github.com/pkmn/EPOke): An enhanced client library which tracks not only the
    observed state of the battle, but also uses the mechanics of the game, reverse damage
    calculation, and usage statistics to infer as much as possible about the state of play.
- identifier: Norström
  active: [2019, 2019]
  paper: Norström:2020
  language: Python
  description: |
    Norström compared the [$`RandomPlayer`](/glossary#RandomPlayer), learned linear combinations of
    features, and Monte Carlo tree search across various scenarios in Generation I including a
    limited version of the OU format implemented with a custom game engine. While MCTS proved to be
    the strongest in relative terms across the scenarios tested, strength in absolute terms was
    unimpressive. Combined approaches and increased engine and training performance were cited as
    the most promising avenues for improvement for future work.
- name: poke-env
  framework: true
  site: https://poke-env.readthedocs.io/en/latest/
  active: 2019 # August 6
  license: MIT
  source: https://github.com/hsahovic/poke-env
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: "0.8.0"
    url: https://github.com/hsahovic/poke-env/releases/tag/0.8.0
  description: |
    The premier Python reinforcement learning framework for competitive Pokémon AIs, poke-env
    supports both singles & doubles battles and random & non-random Generation IV-IX formats.
    [Documentation](https://poke-env.readthedocs.io/en/stable/index.html) and
    [examples](https://github.com/hsahovic/poke-env/tree/46fa2b01/examples) help make it easy for
    beginners to get started, though poke-env's fully-featured [client
    representation](https://github.com/hsahovic/poke-env/tree/46fa2b01/src/poke_env/environment) makes
    it valuable to any Python project.

    poke-env doesn't embed Pokémon Showdown so can't easily perform simulations (or leverage the
    simulator for damage calculations), but handles connecting to a local Pokémon Showdown instance
    and includes [data](https://github.com/hsahovic/poke-env/tree/46fa2b01/src/poke_env/data/static)
    [extracted](https://github.com/hsahovic/poke-env/tree/46fa2b01/scripts) from the simulator as
    well as [team parsing and
    packing](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/teambuilder/teambuilder.py)
    logic to  provide the foundation most agents need.

    Beyond this, poke-env also offers support for training agents
    [concurrently](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/concurrency.py)
    and [utilities for evaluating agent
    performance](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/utils.py)
    (_e.g._, cross evaluation). While [self-play support is still
    experimental](https://github.com/hsahovic/poke-env/blob/46fa2b01/examples/experimental-self-play.py),
    poke-env provides a
    [`MaxBasePowerPlayer`](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/baselines.py#L11)
    and
    [`SimpleHeuristicsPlayer`](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/baselines.py#L19)
    (both simplified versions of an actual [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer)) for
    training. Finally, poke-env contains an [OpenAI Gym](https://github.com/openai/gym)
    [environment
    implementation](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/openai_api.py)
    and
    [helpers](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/env_player.py#L106)
    including a [reward
    helper](https://github.com/hsahovic/poke-env/blob/46fa2b01/src/poke_env/player/env_player.py#L106).
- name: Simplified Pokemon Environment
  active: [2019, 2020] # October 31
  paper: Simoes:2021
  source: https://gitlab.com/DracoStriker/simplified-pokemon-environment
  engine: Custom
- name: VGC AI Framework
  framework: true
  active: 2019 # October 31
  paper: Reis:2021
  license: MIT
  source: https://gitlab.com/DracoStriker/pokemon-vgc-engine
  engine:
    - name: PokemonBattleEngine
      url: https://github.com/Kermalis/PokemonBattleEngine
- name: Athena
  active: [2020, 2023]
  paper: Sarantinos:2023
  language: [TypeScript, C++]
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- active: [2020, 2022] # May 16
  paper: Yu:2020
  source: https://github.com/blue-sky-sea/Pokemon-MCTS-AI-Master
  engine:
    - name: Pokemon-Python
      url: https://github.com/DanielAlcocerSoto/Pokemon-Python
  language: Python
- active: [2020, 2020] # June 13
  source: https://github.com/anthonykrivonos/pokemon-ai
  engine: Custom
- name: Future Sight
  site: https://www.pokemonbattlepredictor.com/FSAI
  active: 2020 # August 16
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: PokeSim
      url: https://github.com/aed3/poke-sim
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: BigBoy
  active: [2021, 2022] # April 21
  source: https://github.com/attraylor/poke-env
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- active: [2021, 2021] # October 15
  source: https://github.com/leolellisr/poke_RL
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: Youngster Joey
  active: [2021, 2021] # November 10
  paper: Kyler-Wank:2021
  source: https://github.com/alex-shen1/Youngster-Joey
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: damage-calc
      url: https://github.com/smogon/damage-calc
  language: JavaScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    Youngster Joey targeted a perfect information variant of Generation I with a greedy policy that
    determines its action by maximizing a set of hand-crafted
    [heuristics](https://github.com/alex-shen1/Youngster-Joey/blob/85e7f63d/bot.js#L226-310). A
    slightly more sophisticated [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer), Younger Joey
    deliberately eschewed minimax and expectiminimax due to concerns over the game tree size even
    after pruning and the algorithms not being able to play optimally against opponents who were
    also expected to play suboptimally. The addition of some form of lookahead was considered for
    future work.
- name: alphaPoke
  site: https://matteoh2o1999.github.io/en-us/projects/alphaPoke-project-delivery
  active: 2021 # November 22
  paper: Dell’Acqua:2022
  license: GPL-3.0
  source: https://github.com/MatteoH2O1999/alphaPoke
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
      name: Project Delivery
      url: https://github.com/MatteoH2O1999/alphaPoke/releases/tag/delivery
- identifier: Tse
  active: [2022, 2022]
  paper: Tse:2022
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    Tse built a Generation VIII VGC agent that leveraged both supervised learning and deep
    Q-learning with [poke-env](#poke-env) to achieve success against
    [$`RandomPlayer`](/glossary#RandomPlayer) and [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer)
    baselines.

    Tse used a dataset of 700,000 Pokémon Showdown server battle logs gathered from the top 20% of
    ranked VGC players on the site to train a 6-layer neural network with supervised learning. The
    network's output layer consisted of two softmax units producing action and value estimates, with
    a move selection and move target being considered separately. Despite using categorical
    encodings for the input embedding (as opposed to a one-hot key embedding) the initial input size
    of of 4186 features resulted in overfitting. As a result, Tse's used the supervised network to
    learn a smaller embedding of 254 features based on the output of the first layer which was
    designed to not be fully connected so as to be used for this purpose.

    The learned embedding was used as input to a second RL network which used deep Q-learning and an
    ε-greedy policy to learn action selection. The reward for this network was computed based on a
    linear combination of the battle outcome, the current percentage HP for both sides' active
    Pokémon, and the value output from the supervised learning network. The training of the RL
    network was performed against the $`RandomPlayer` and $`MaxDamagePlayer` and this led to a
    learned policy which proved to be fairly exploitable by less contrived opponents.

    Battle simulation performance was cited as the main bottleneck, discouraging the addition of
    Monte Carlo tree search and serving as a barrier to increased training time.
- name: Pinyon
  framework: true
  active: 2022 # May 20
  license: GPL-2.0
  source: https://github.com/baskuit/pinyon
  language: C++
- name: Meloetta
  framework: true
  active: [2022, 2023] # December 21
  source: https://github.com/spktrm/meloetta
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: pokemon-showdown-client
      url: https://github.com/smogon/pokemon-showdown-client
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  release:
    name: "1.1.0"
    url: https://pypi.org/project/meloetta/1.1.0/
- name: Dolly
  active: [2023, 2023] # January 8
  license: Apache-2.0
  source: https://github.com/baskuit/dolly
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: Pokemon Trainer AI
  active: [2023, 2023] # March 28
  engine: Custom
  source: https://github.com/FredodaFred/pokemon-battle-ai
  language: Python
  description: |
    Pokemon Trainer AI used a [bespoke Generation I
    engine](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/classes.py#L462) to
    evaluate several different agents in a level 50 3v3 format:

    - [RBES](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/RBES.png), a rules-based
    expert system that decided on actions based on its [hand-crafted evaluation
    function](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/InferenceEngine.py)
    that primarily focused on maximizing damage.
    - [DT](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/DT.png), a decision tree
    based approach
    [produced](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/pokemon_DT_solution.ipynb)
    for a specific team configuration.
    - [RL](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/RL.png), a state-based
    Q-learning model also
    [trained](https://github.com/FredodaFred/pokemon-battle-ai/blob/af1446df/RL_train.py) for a
    specific team configuration.
- name: Pokesim
  framework: true
  active: 2023 # October 2
  source: https://github.com/spktrm/pokesim
  engine:
    - name: '@pkmn/sim'
      url: https://github.com/pkmn/ps/tree/main/sim
    - name: '@pkmn/client'
      url: https://github.com/pkmn/ps/tree/main/client
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
- name: Pokémon Simulator Environment
  active: 2023
  source: https://github.com/cRz-Shadows/Pokemon_Trainer_Tournament_Simulator
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
  language: TypeScript
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown
  description: |
    The Pokémon Simulator Environment is an environment for running a large number of Pokémon
    Showdown battle simulations. While not intended as a serious attempt at a competitive Pokémon
    battling agent, the project features an elaborate [rules-based heuristic
    agent](https://github.com/cRz-Shadows/pokemon-showdown/blob/5932a450/sim/examples/Simulation-test-1.ts)
    that can serve as a training baseline for other agents in singles formats. Supporting all
    generations, the `HeuristicPlayerAI`'s extends past the
    [$`MaxDamagePlayer`](/glossary#MaxDamagePlayer), considering elements beyond just the potential
    damage its moves may do. The agent evaluates
    [matchups](https://github.com/cRz-Shadows/pokemon-showdown/blob/5932a450/sim/examples/Simulation-test-1.ts#L263-L287)
    based on type advantage, speed, and remaining HP percentage, considers [switching
    out](https://github.com/cRz-Shadows/pokemon-showdown/blob/5932a450/sim/examples/Simulation-test-1.ts#L373-L400),
    and contains handwritten cases to handle specific game mechanics or to be able to use moves
    which indirectly benefit the player.
- paper: Zhang:2024
  active: [2023, 2023] # November 29
  source: https://github.com/alexzhang13/reward-shaping-rl
  engine:
    - name: pokemon-showdown
      url: https://github.com/smogon/pokemon-showdown
    - name: poke-env
      url: https://github.com/hsahovic/poke-env
  language: Python
  platform:
    - name: Pokémon Showdown!
      url: https://github.com/smogon/pokemon-showdown